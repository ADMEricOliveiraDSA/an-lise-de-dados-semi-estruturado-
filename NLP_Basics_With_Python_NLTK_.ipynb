{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "NLP_Basics_With_Python_NLTK .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricOliveira17/Data-Science-Estudos/blob/master/NLP_Basics_With_Python_NLTK_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzzutoBPvrrd"
      },
      "source": [
        "# PROCESSAMENTO DE LINGUAGEM NATURAL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbrnQeFOvrrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7df71f-41f0-486f-c2c0-a4ed632cfd10"
      },
      "source": [
        "# IMPORTAÇÃO PACOTES E DEPENDÊNCIAS \n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3A6qqd9vrsZ"
      },
      "source": [
        "### Tokenization  (Bag-of-words)\n",
        "#### processo de quebrar em \"tokens\" ou seja ocorre a quebra do texto em termos individuais. Considere a frase : \"Eu sou muito esforçado\". O processo observa os espaços em branco dessa sentença e determina os tokens:[\"Eu\",\"sou\",\"muito\",\"esforçado\"]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRWz052Xvrsc",
        "outputId": "38c724bf-d868-4cd0-d37c-61e0b14c257a"
      },
      "source": [
        "# CRIAR TEXTO DE EXEMPLO \n",
        "word_data   = \"Eu sou muito esforçado\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print (nltk_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Eu', 'sou', 'muito', 'esforçado']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gXaYLptxaK9"
      },
      "source": [
        "##Tokenização de sentenças \n",
        "### Também podemos tokenizar as frases em um parágrafo como fizemos com as palavras.  Usamos o método sent_tokenize para fazer isso.  Abaixo está um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huNqMBqUvrsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e9a612-aa98-4b26-c5d2-9e3466dad00c"
      },
      "source": [
        "sentence_data = \"Busco melhorias contínuas. Foco no objetivo.\"\n",
        "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
        "print (nltk_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Busco melhorias contínuas.', 'Foco no objetivo.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCPfdkcfzz88"
      },
      "source": [
        "## Stemming \n",
        "### o processo de stemming consiste em reduzir uma palavra ou termo ao seu radical. Um determinado o texto tem uma coleção de documentos \"que são instâncias ou registros de uma tabela não normalizada\" dos seguintes termos: planejamento,planejei, planejado, planejar. Após a aplicação do processo de stemming, todos os termos derivados de planejar, seriam reproduzidas ao seu radical \"planej\" e não gerariam mais redundâncias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBUJ_07By8-a",
        "outputId": "d169db58-ce4c-4338-df5c-8362ea6f3510"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        " \n",
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "# Tokenização da primeira palavra / funciona melhor com inglês! \n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "# Para cada palavra em nltk_tokens, imprimir na tela as transformações  \n",
        "for w in nltk_tokens:\n",
        "    print(\"Atual: %s Stemming: %s\" % (w,porter_stemmer.stem(w)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Atual: It Stemming: It\n",
            "Atual: originated Stemming: origin\n",
            "Atual: from Stemming: from\n",
            "Atual: the Stemming: the\n",
            "Atual: idea Stemming: idea\n",
            "Atual: that Stemming: that\n",
            "Atual: there Stemming: there\n",
            "Atual: are Stemming: are\n",
            "Atual: readers Stemming: reader\n",
            "Atual: who Stemming: who\n",
            "Atual: prefer Stemming: prefer\n",
            "Atual: learning Stemming: learn\n",
            "Atual: new Stemming: new\n",
            "Atual: skills Stemming: skill\n",
            "Atual: from Stemming: from\n",
            "Atual: the Stemming: the\n",
            "Atual: comforts Stemming: comfort\n",
            "Atual: of Stemming: of\n",
            "Atual: their Stemming: their\n",
            "Atual: drawing Stemming: draw\n",
            "Atual: rooms Stemming: room\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biCiUItN6VZF"
      },
      "source": [
        " ## Remoção de stopwords \n",
        "### Sabe aqueles termos que aparecem no texto que têm funções meramente de conectar elementos de uma sentença ? São stopwords! Artigos(o,a,os,um,umas, etc.),crase (à),pronomes(ele,teu,meu),preposições(de,para,entre,etc.),são termos que não são relevantes para a análise e por isso são removidos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul8Q52g28uTc",
        "outputId": "de4cdf82-a123-4bad-fb07-4017a136b156"
      },
      "source": [
        " from nltk.corpus import stopwords\n",
        "# CRIAR TEXTO DE EXEMPLO \n",
        "texto =\" Eu hoje tenho um problema difícil e sei que tenho que me esforçar para resolver\"\n",
        " \n",
        "# PASSA AS SENTENÇAS AO ALGORITMO \n",
        "sentences = nltk.sent_tokenize(texto)\n",
        " \n",
        "# PARA CADA ITEM DENTRO DA SENTENÇA, APLICA REMOÇÃO E RETORNA O ITEM \n",
        "# SE HAVER STOPWORD EM PT-BR RETIRA,E RECEBE A SENTENÇA\n",
        "# SENTENÇA VERDADEIRA RECEBE INCREMENTO DAS NOVAS PALAVRAS \n",
        " \n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    newwords = [word for word in words if word not in stopwords.words('portuguese')]\n",
        "    sentences[i] = ' '.join(newwords)\n",
        "    \n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Eu hoje problema difícil sei esforçar resolver']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}